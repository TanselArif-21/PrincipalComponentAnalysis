
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{PCA1}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Principal Component
Analysis}\label{principal-component-analysis}

    \subsubsection{1- Description}\label{description}

    \begin{itemize}
\tightlist
\item
  If the dataset includes a large number of variables, it can be
  difficult to visualise it
\item
  It is useful to truncate the number or variables into a small number
  of dimensions in order to be able to visualise the most critical
  dimensions
\item
  This means that we are looking for a representation of the data in a
  lower dimensional space
\item
  The task is to find representative dimensions for the dataset derived
  from a linear combination of the original variables and order them
  from the most important to the least important
\item
  This is what Principal Component Analysis (PCA) aims to accomplish
\end{itemize}

    \subsubsection{2- Iris Dataset}\label{iris-dataset}

    \begin{itemize}
\tightlist
\item
  The Iris dataset consists of 3 different kinds of irises (Setosa,
  Versicolor, and Virginica)
\item
  The variables are petal and sepal length and width
\item
  The classes correspond to 0 - Setosa, 1 - Versicolor, 2 - Virginica
\end{itemize}

    First we import the iris dataset

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} Import a dummy dataset (iris)}
        \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{datasets}
        
        \PY{c+c1}{\PYZsh{} Load the dataset}
        \PY{n}{iris} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{load\PYZus{}iris}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    We then import any modules we may use

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} Import modules}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}


    We now create the data frames we will need

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} Create the features and class data frames}
        \PY{n}{iris\PYZus{}features} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{iris}\PY{o}{.}\PY{n}{data}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{n}{iris}\PY{o}{.}\PY{n}{feature\PYZus{}names}\PY{p}{)}
        \PY{n}{iris\PYZus{}class} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{iris}\PY{o}{.}\PY{n}{target}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Create a complete data frame}
        \PY{n}{iris\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{iris\PYZus{}features}\PY{p}{,}\PY{n}{iris\PYZus{}class}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    It is useful to understand the data we are dealing with. In the above,
iris\_features is a dataframe consisting only of the explanatory
variables - sepal length (cm), sepal width (cm), petal length (cm) and
petal width (cm) - whereas the iris\_class dataframe consists of only
the response variable - Class.

A pairplot is useful in quickly investigating any relationships between
the variables. We can see from the pairplot below that petal width and
petal length are correlated and either are an indicator to the class of
the iris. However this is not true for the sepal width and sepal length.
It can be seen that a large sepal width and a small sepal length may be
indicative of class 2 - Virginica. We therefore expect that there exists
a principal component with these two variables having opposite signs.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} Create a pairplot of each variable to identify if there is the class is easily distinguishable}
        \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{iris\PYZus{}data}\PY{p}{,}\PY{n}{hue}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:} <seaborn.axisgrid.PairGrid at 0x27e7e793eb8>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_12_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{3- Principal Component Analysis
(PCA)}\label{principal-component-analysis-pca}

    The idea behind PCA is the determination of orthonormal dimensions such
that each dimension captures an axis with approximately maximum
variance. For example in a simplified plot below of petal length, petal
width and class, the maximum variance is obtained approximately along
the petal width = petal length line, since the projections of the points
onto this line has the largest variance.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k}{def} \PY{n+nf}{colorify}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{n}{x}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{:}
                \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}
            \PY{k}{elif} \PY{n}{x}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{:}
                \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}
            \PY{k}{else}\PY{p}{:}
                \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}
        
        \PY{c+c1}{\PYZsh{} Create a color array to attach a color to each point depending on its label}
        \PY{n}{color} \PY{o}{=} \PY{n}{iris\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{colorify}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{sns}\PY{o}{.}\PY{n}{jointplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{iris\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal length (cm)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{n}{iris\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal width (cm)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{joint\PYZus{}kws}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{color}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{n}{color}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{,}\PY{n}{marginal\PYZus{}kws}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bins}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{50}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:} <seaborn.axisgrid.JointGrid at 0x27e0342a4e0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_16_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Let there be \(p\) variables in our dataset with
\(\bf{X} = (\bf{X_1},\bf{X_2},...,\bf{X_p})^T\) a column vector of
random variables representing the variables of the dataset. A principal
component, \(P_1\), then has the form

\[P_1 = \alpha_{11} \textbf{X}_1 + \alpha_{12} \textbf{X}_2 + ... + \alpha_{1p} \textbf{X}_p = \sum_{i=1}^p \alpha_{1i} \textbf{X}_i = (\alpha_{11},\alpha_{12},...,\alpha_{1p})\textbf{X} = \boldsymbol{\alpha}_{1}^T \textbf{X}\]

Then the \emph{first principal component} is the linear combination of
the variables which results in the greatest variance. The variance of
\(P_1\) can be calculated as

\[Var[P_1] = \text{E}[\textbf{P}_1^2] - \text{E}[\textbf{P}_1]^2 = \boldsymbol{\alpha}_{1}^T \boldsymbol{\sum} \boldsymbol{\alpha}_{1}\]

where \(\boldsymbol{\sum}\) is the \(p \times p\) covariance matrix of
\(\textbf{X}\). This result is shown in Appendix A.1.

Since \(P_1\), the \emph{first principal component}, is the principal
component with the greatest variance, we will need to maximise
\(Var[P_1]\) in terms of \(\alpha_i\). i.e.

\[\max_{\alpha_i} P_1(\alpha_i,\boldsymbol{X})\]

The initial point here is that \(P_1\) is just a linear combination of
the variables, so nothing stops the unbounded variables, \(\alpha_i\)
from going to infinity. We solve this issue by adding the constraint

\[\sum_{i} \alpha_{1i}^2 = \boldsymbol{\alpha}_1^T \boldsymbol{\alpha}_1 =  1\]

Given this bound, the coefficients will have to compete with each other
to maximise \(P_1\). Note: A constraint of the form
\(\sum_{i} \alpha_{1i} = 1\) is not sufficient as it doesn't solve the
issue of unboundedness.

To calculate \(Var(P_1)\), as it is currently written, we will need to
know the covariance matrix \(\sum\). However, often we are dealing with
samples and do not know the covariance matrix. An unbiased, Maximum
Likelihood Estimator (MLE) for the covariance matrix of the data
\(\boldsymbol{X}\) is

\[\boldsymbol{S} = \frac{1}{n-1} \sum_{i=1}^n(\boldsymbol{x}_i - \hat{\boldsymbol{\mu}}) (\boldsymbol{x}_i - \hat{\boldsymbol{\mu}})^T\]

where \(\boldsymbol{X}\) is a \(p \times n\) data matrix,
\(\boldsymbol{x}_i\) is the \(p \times 1\) column vector for observation
\(i\), and
\(\hat{\boldsymbol{\mu}} = \frac{1}{n} \sum_{i=1}^n \boldsymbol{x}_i\)
is the \(p \times 1\) unbiased MLE for the mean \(\boldsymbol{\mu}\)
where \(\mu_i\) is the mean value of the ith parameter/variable.

Sometimes \(\boldsymbol{S}\) is expressed as

\[\boldsymbol{S} =  \frac{1}{n-1} \boldsymbol{X}^T \boldsymbol{X}\]

where \(\boldsymbol{X}\) is already mean centered.

We therefore need to maximise

\[\boldsymbol{\alpha}_{1}^T \boldsymbol{S} \boldsymbol{\alpha}_{1}\]

with the constraint

\[\boldsymbol{\alpha}_1^T \boldsymbol{\alpha}_1 =  1\].

We can do this via \emph{Lagrange Multipliers}. Let
\(\mathcal{L}(\boldsymbol{\alpha_1},\lambda_1)\) be the Lagrange
function defined by

\[\mathcal{L}(\boldsymbol{\alpha}_1,\lambda_1) = f(\boldsymbol{\alpha}_1) - \lambda_1 g(\boldsymbol{\alpha}_1)\]

where \(f(\boldsymbol{\alpha}_1)\) is the function to be
maximised/minimised and \(g(\boldsymbol{\alpha}_1)\) is a constraint. We
need to solve

\(\nabla_{\boldsymbol{\alpha}_1,\lambda_1}\mathcal{L}(\boldsymbol{\alpha_1},\lambda_1) = 0\)
which in turn implies solving
\(\nabla_{\boldsymbol{\alpha}_1}f(\boldsymbol{\alpha}_1) = \lambda_1 \nabla_{\boldsymbol{\alpha}_1} g(\boldsymbol{\alpha}_1)\)
and \(g(\boldsymbol{\alpha}_1) = 0\).

In our particular case we have

\[\mathcal{L}(\boldsymbol{\alpha}_1,\lambda_1) = \boldsymbol{\alpha}_{1}^T \boldsymbol{S} \boldsymbol{\alpha}_{1} - \lambda_1 (\boldsymbol{\alpha}_1^T \boldsymbol{\alpha}_1 - 1)\]

Then

\[\nabla \mathcal{L}(\boldsymbol{\alpha}_1,\lambda_1) = 2 \boldsymbol{S} \boldsymbol{\alpha}_1 - 2 \lambda_1 \boldsymbol{\alpha}_1 = 0\]

Resulting in the eigenvalue equation

\[\boldsymbol{S} \alpha_1 = \lambda_1 \boldsymbol{\alpha}_1 \implies \lambda_1 \text{is a solution to det}(\boldsymbol{S} - \lambda \boldsymbol{I}) = 0\]

This has at most \(p\) roots. Since we're looking for the \(P_1\) which
is the solution resulting in the largest variance, we choose the largest
of these roots since \(\lambda_1\) is related to \(Var(P_1)\). This can
be seen by multiplying the eigenvalue equation above on both sides by
\(\boldsymbol{\alpha}_1\) to obtain \(Var(P_1)\)

\[\boldsymbol{\alpha}_1^T \boldsymbol{S} \boldsymbol{\alpha}_1 = \lambda_1\]

We first calculate \(\boldsymbol{S}\) for our dataset.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k}{def} \PY{n+nf}{VectorScalarSubtract}\PY{p}{(}\PY{n}{u}\PY{p}{,}\PY{n}{v}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{l+s+sd}{    Subtracts scalar v from vector u.}
        \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{k}{return} \PY{n+nb}{len}\PY{p}{(}\PY{n}{u}\PY{p}{)}
        
        \PY{k}{def} \PY{n+nf}{VectorSubtract}\PY{p}{(}\PY{n}{u}\PY{p}{,}\PY{n}{v}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{l+s+sd}{    Subtracts vector v from vector u.}
        \PY{l+s+sd}{    Vectors must be of the same size.}
        \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{n}{tempArray} \PY{o}{=} \PY{n}{u}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{]}
            
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{u}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{n}{tempArray}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{u}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{p}{]}
            
            \PY{k}{return} \PY{n}{tempArray}
        
        \PY{k}{def} \PY{n+nf}{MatrixScalarMult}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{s}\PY{p}{)}\PY{p}{:}
            \PY{n}{rows} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            \PY{n}{cols} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}
            \PY{n}{temp} \PY{o}{=} \PY{n}{X}
            
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{rows}\PY{p}{)}\PY{p}{:}
                \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{cols}\PY{p}{)}\PY{p}{:}
                    \PY{n}{temp}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{o}{*}\PY{n}{s}
                    
            \PY{k}{return} \PY{n}{temp}
        
        \PY{k}{def} \PY{n+nf}{DotProduct}\PY{p}{(}\PY{n}{u}\PY{p}{,}\PY{n}{v}\PY{p}{)}\PY{p}{:}
            \PY{n}{total} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{u}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{n}{total} \PY{o}{+}\PY{o}{=} \PY{n}{u}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{*}\PY{n}{v}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                
            \PY{k}{return} \PY{n}{total}
        
        \PY{k}{def} \PY{n+nf}{MultMatrix}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y}\PY{p}{)}\PY{p}{:}
            \PY{n}{p} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}
            \PY{n}{obs} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            \PY{n}{temparray} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{Z} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{p}\PY{p}{)}\PY{p}{:}
                \PY{n}{u} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                \PY{n}{temparray} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{p}\PY{p}{)}\PY{p}{:}
                    \PY{n}{v} \PY{o}{=} \PY{n}{Y}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{n}{j}\PY{p}{]}
                    \PY{n}{temparray}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{DotProduct}\PY{p}{(}\PY{n}{u}\PY{p}{,}\PY{n}{v}\PY{p}{)}\PY{p}{)}
                
                \PY{n}{Z}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{temparray}\PY{p}{)}
                
            \PY{k}{return} \PY{n}{Z}
        
        \PY{k}{def} \PY{n+nf}{CenterMatrix}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
            \PY{n}{obs} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            \PY{n}{params} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
            \PY{n}{tempArray} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            
            \PY{n}{mu} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{params}\PY{p}{)}\PY{p}{:}
                \PY{n}{v} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{o}{/}\PY{n}{obs}
                \PY{n}{mu}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{v}\PY{p}{)}
                
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{obs}\PY{p}{)}\PY{p}{:}
                \PY{n}{a} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{p}{:}\PY{p}{]}
                \PY{n}{tempArray}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{VectorSubtract}\PY{p}{(}\PY{n}{a}\PY{p}{,}\PY{n}{mu}\PY{p}{)}\PY{p}{)}
                
            \PY{k}{return} \PY{n}{tempArray}
                
        \PY{k}{def} \PY{n+nf}{CalcCovarMatrix}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
            \PY{n}{obs} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            
            \PY{n}{M} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{CenterMatrix}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}
            \PY{n}{MT} \PY{o}{=} \PY{n}{M}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
                
            \PY{n}{Z} \PY{o}{=} \PY{n}{MultMatrix}\PY{p}{(}\PY{n}{MT}\PY{p}{,}\PY{n}{M}\PY{p}{)} 
            
            \PY{n}{Z} \PY{o}{=} \PY{n}{MatrixScalarMult}\PY{p}{(}\PY{n}{Z}\PY{p}{,}\PY{l+m+mi}{1}\PY{o}{/}\PY{p}{(}\PY{n}{obs}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            
            \PY{k}{return} \PY{n}{Z}
        
        \PY{k}{def} \PY{n+nf}{Determinant}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
            \PY{n}{det} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{2}\PY{p}{:}
                \PY{k}{return} \PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{\PYZhy{}} \PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} Go along the top row}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{}print(GetSubMatrix(X,0,i))}
                    \PY{c+c1}{\PYZsh{}print(Determinant(GetSubMatrix(X,0,i)))}
                    
                    \PY{n}{det} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{n}{i} \PY{o}{*} \PY{n}{X}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{i}\PY{p}{]} \PY{o}{*} \PY{n}{Determinant}\PY{p}{(}\PY{n}{GetSubMatrix}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{i}\PY{p}{)}\PY{p}{)}
                \PY{k}{return} \PY{n}{det}
        
        \PY{k}{def} \PY{n+nf}{MatrixSubtract}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y}\PY{p}{,}\PY{n}{dp}\PY{p}{)}\PY{p}{:}
            \PY{n}{F} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{X}\PY{p}{)}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{C}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{C}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{n}{F}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{Z}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{C}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{,}\PY{n}{dp}\PY{p}{)}
            
            \PY{k}{return} \PY{n}{F}
        
        \PY{k}{def} \PY{n+nf}{GetSubMatrix}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{n}{j}\PY{p}{)}\PY{p}{:}
            \PY{n}{T} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{T} \PY{o}{=} \PY{n}{T}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{T}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
            
            \PY{k}{return} \PY{n}{T}
        
        \PY{k}{def} \PY{n+nf}{power\PYZus{}iteration}\PY{p}{(}\PY{n}{A}\PY{p}{,} \PY{n}{num\PYZus{}simulations}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Ideally choose a random vector}
            \PY{c+c1}{\PYZsh{} To decrease the chance that our vector}
            \PY{c+c1}{\PYZsh{} Is orthogonal to the eigenvector}
            \PY{n}{b\PYZus{}k} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{A}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
            \PY{n}{l} \PY{o}{=} \PY{l+m+mi}{0}
        
            \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}simulations}\PY{p}{)}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} calculate the matrix\PYZhy{}by\PYZhy{}vector product Ab}
                \PY{n}{b\PYZus{}k1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{A}\PY{p}{,} \PY{n}{b\PYZus{}k}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} calculate the norm}
                \PY{n}{b\PYZus{}k1\PYZus{}norm} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{b\PYZus{}k1}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} re normalize the vector}
                \PY{n}{b\PYZus{}k} \PY{o}{=} \PY{n}{b\PYZus{}k1} \PY{o}{/} \PY{n}{b\PYZus{}k1\PYZus{}norm}
                
            \PY{n}{l} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{b\PYZus{}k}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{A}\PY{p}{,}\PY{n}{b\PYZus{}k}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{b\PYZus{}k}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{b\PYZus{}k}\PY{p}{)}\PY{p}{)}
        
            \PY{k}{return} \PY{n}{b\PYZus{}k}\PY{p}{,}\PY{n}{l}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{c+c1}{\PYZsh{} The Iris dataset}
         \PY{n}{X} \PY{o}{=} \PY{n}{iris\PYZus{}features}
         
         \PY{c+c1}{\PYZsh{} Calculate the covariance matrix for our dataset}
         \PY{n}{Z} \PY{o}{=} \PY{n}{CalcCovarMatrix}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         \PY{n}{Z} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{Z}\PY{p}{)}
         
         \PY{n}{Z}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}38}]:}           0         1         2         3
         0  0.685694 -0.039268  1.273682  0.516904
         1 -0.039268  0.188004 -0.321713 -0.117981
         2  1.273682 -0.321713  3.113179  1.296387
         3  0.516904 -0.117981  1.296387  0.582414
\end{Verbatim}
            
    The calculation of the covariance matrix can also be done using numpy
with the np.cov() method as follows (we see that we have the same
result)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n}{C} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{matrix}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
         \PY{n}{C} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{cov}\PY{p}{(}\PY{n}{C}\PY{p}{)}\PY{p}{)}
         \PY{n}{C}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}39}]:}           0         1         2         3
         0  0.685694 -0.039268  1.273682  0.516904
         1 -0.039268  0.188004 -0.321713 -0.117981
         2  1.273682 -0.321713  3.113179  1.296387
         3  0.516904 -0.117981  1.296387  0.582414
\end{Verbatim}
            
    The eigenvector of the covariance matrix corresponding to the eigenvalue
with the greatest magnitude can be iteratively calculated using the
Power Iteration method (for now taken from wikipedia)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n}{a\PYZus{}1}\PY{p}{,}\PY{n}{l\PYZus{}1} \PY{o}{=} \PY{n}{power\PYZus{}iteration}\PY{p}{(}\PY{n}{Z}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}


    The eigenvalue is

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{l\PYZus{}1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
4.224840768320114

    \end{Verbatim}

    And the corresponding eigenvector (for the \emph{first principal
component}) is

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{a\PYZus{}1}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}43}]:} array([ 0.36158968, -0.08226889,  0.85657211,  0.35884393])
\end{Verbatim}
            
    The eigenvalue, \(a_1\), and eigenvector, \(v\) are by definition a
solution to the equation

\[\boldsymbol{S} \boldsymbol{a}_1 = l_1 \boldsymbol{a}_1\]

We can check that this is indeed true

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{n}{np}\PY{o}{.}\PY{n}{subtract}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{Z}\PY{p}{,}\PY{n}{a\PYZus{}1}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{l\PYZus{}1}\PY{p}{,}\PY{n}{a\PYZus{}1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}44}]:} array([0., 0., 0., 0.])
\end{Verbatim}
            
    We now need to find the \emph{second principal component}

\[P_2 = \boldsymbol{\alpha}_2^T \boldsymbol{X}\]

However, the difference with what we did before is that the eigenvector
for \(P_2\) has to be orthogonal to that for \(P_1\). i.e.

\[\boldsymbol{\alpha}_1^T \boldsymbol{\alpha}_2 = 0\]

This constraint along with the constraint analoguous to the previous
case

\[\boldsymbol{\alpha}_2^T \boldsymbol{\alpha}_2 = 1\]

allows the discovery of an eigenvector that is different from
\(\boldsymbol{\alpha}_1\)

This results in the lagrange equation (maximising this corresponds to
minimising \(\boldsymbol{\alpha}_1^T \boldsymbol{\alpha}_2\))

\[\mathcal{L}(\boldsymbol{\alpha}_2,\lambda_2,r)=f(\boldsymbol{\alpha}_2) - \lambda_2 g(\boldsymbol{\alpha}_2) - r \boldsymbol{\alpha}_1^T \boldsymbol{\alpha}_2\]

Resulting in

\[\nabla \mathcal{L} = 2 \boldsymbol{S} \alpha_2 - 2 \lambda_2 \boldsymbol{\alpha}_2 - r \boldsymbol{\alpha}_1\]

Setting this to 0

\[\boldsymbol{S} \boldsymbol{\alpha}_2 - \lambda_2 \boldsymbol{\alpha}_2 = \frac{1}{2} r \boldsymbol{\alpha}_1\]

multiplying both sides by \(\boldsymbol{\alpha}_1^T\)

\[\boldsymbol{\alpha}_1^T \boldsymbol{S} \boldsymbol{\alpha}_2 - \lambda_2 \boldsymbol{\alpha}_1^T \boldsymbol{\alpha}_2 = \frac{1}{2} r \boldsymbol{\alpha}_1^T \boldsymbol{\alpha}_1\]

\[\implies \boldsymbol{\alpha}_1^T \boldsymbol{S} \boldsymbol{\alpha}_2 = \frac{1}{2} r\]

where \(\boldsymbol{\alpha}_1^T \boldsymbol{\alpha}_2 = 0\) and
\(\boldsymbol{\alpha}_1^T \boldsymbol{\alpha}_1 = 1\)

Remembering that

\[\boldsymbol{S} \boldsymbol{\alpha}_1 = \lambda_1 \boldsymbol{\alpha}_1\]

and multiplying both sides by \(\boldsymbol{\alpha}_2^T\)

\[\boldsymbol{\alpha}_2^T \boldsymbol{S} \boldsymbol{\alpha}_1 = \lambda_1 \boldsymbol{\alpha}_2^T \boldsymbol{\alpha}_1 = 0\]

This means that \(r=0\). Finally,

\[\boldsymbol{S} \boldsymbol{\alpha}_2 = \lambda_2 \boldsymbol{\alpha}_2 \implies \boldsymbol{\alpha}_2^T \boldsymbol{S} \boldsymbol{\alpha}_2 = \lambda_2\]

meaning that \(\lambda_2\) is related to the variance corresponding to
the \emph{second principal component} and must be the second largest in
magnitude.

Here we can use the numpy.linalg module to calculate the eigenvalues for
our covariance matrix.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{k+kn}{from} \PY{n+nn}{numpy} \PY{k}{import} \PY{n}{linalg} \PY{k}{as} \PY{n}{LA}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n}{L}\PY{p}{,}\PY{n}{E} \PY{o}{=} \PY{n}{LA}\PY{o}{.}\PY{n}{eig}\PY{p}{(}\PY{n}{Z}\PY{p}{)}
         \PY{n}{E} \PY{o}{=} \PY{n}{E}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Eigenvectors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{E}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Eigenvalues}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{L}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Eigenvectors
[[ 0.36158968 -0.08226889  0.85657211  0.35884393]
 [-0.65653988 -0.72971237  0.1757674   0.07470647]
 [-0.58099728  0.59641809  0.07252408  0.54906091]
 [ 0.31725455 -0.32409435 -0.47971899  0.75112056]]
Eigenvalues
[4.22484077 0.24224357 0.07852391 0.02368303]

    \end{Verbatim}

    In order to check that this is indeed the eigenvalue/eigenvector
decomposition of \(Z\), we test the following equation

\[\boldsymbol{E} \boldsymbol{Z} = \boldsymbol{\Lambda} \boldsymbol{E}^T\]

where
\(\boldsymbol{E} = (\boldsymbol{\alpha}_1,\boldsymbol{\alpha}_2,...,\boldsymbol{\alpha}_p)^T\)
is the matrix of eigenvectors and \(\boldsymbol{\Lambda}\) is a diagonal
matrix where \(\Lambda_{ii}\) is the \(i\)th eigenvalue.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{c+c1}{\PYZsh{} The diagonal matrix}
         \PY{n}{LI} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{identity}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{:}
             \PY{n}{LI}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{LI}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{n}{L}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{LI}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}47}]:} array([[4.22484077, 0.        , 0.        , 0.        ],
                [0.        , 0.24224357, 0.        , 0.        ],
                [0.        , 0.        , 0.07852391, 0.        ],
                [0.        , 0.        , 0.        , 0.02368303]])
\end{Verbatim}
            
    We can see below that we have approximate equality

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{subtract}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{E}\PY{p}{,}\PY{n}{Z}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{LI}\PY{p}{,}\PY{n}{E}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[[-2.22044605e-16  8.32667268e-16  2.66453526e-15  6.66133815e-16]
 [-4.71844785e-16  1.66533454e-16 -8.04911693e-16 -5.23886490e-16]
 [-2.84494650e-16  1.59594560e-16 -4.47558657e-16 -9.71445147e-17]
 [ 4.33680869e-18 -2.51534904e-17 -1.21430643e-16 -7.28583860e-17]]

    \end{Verbatim}

    The principal components are then as follows:

\[P_1 = \alpha_{11} X_1 + \alpha_{12} X_2 + \alpha_{13} X_3 + \alpha_{14} X_4 + \alpha_{15} X_5\]
\[P_2 = \alpha_{21} X_1 + \alpha_{22} X_2 + \alpha_{23} X_3 + \alpha_{24} X_4 + \alpha_{25} X_5\]
\[P_3 = \alpha_{31} X_1 + \alpha_{32} X_2 + \alpha_{33} X_3 + \alpha_{34} X_4 + \alpha_{35} X_5\]
\[P_4 = \alpha_{41} X_1 + \alpha_{42} X_2 + \alpha_{43} X_3 + \alpha_{44} X_4 + \alpha_{45} X_5\]
\[P_5 = \alpha_{51} X_1 + \alpha_{52} X_2 + \alpha_{53} X_3 + \alpha_{54} X_4 + \alpha_{55} X_5\]

This can be written concisely in matrix form as

\[\boldsymbol{K}_{ij} = \sum_{l = 1}^5 X_{il} V_{lj}\]

where
\(\boldsymbol{V} = \boldsymbol{\alpha}^T = (\boldsymbol{\alpha}_1,\boldsymbol{\alpha}_2,...,\boldsymbol{\alpha}_5)\)
and \(\boldsymbol{X}\) is the \(n \times p\) data matrix with \(n\)
observations in \(p=5\) variables.

\(\boldsymbol{K}\) is called the \emph{scores matrix} where \(K_{ij}\)
is the value of the \(j\)th principal component of the \(i\)th
observation and \(\boldsymbol{V}\) is the \emph{loadings matrix}.
\(\boldsymbol{K}\) is the projections of the data onto the principal
components. This means we can plot the columns of the scores matrix
against each other to see how the principal components distinguish the
data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{n}{V} \PY{o}{=} \PY{n}{E}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
         \PY{n}{K} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{V}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{c+c1}{\PYZsh{} This is the first principal component for all 150 observations}
         \PY{n}{K}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}50}]:} array([-2.68420713, -2.71539062, -2.88981954, -2.7464372 , -2.72859298,
                -2.27989736, -2.82089068, -2.62648199, -2.88795857, -2.67384469,
                -2.50652679, -2.61314272, -2.78743398, -3.22520045, -2.64354322,
                -2.38386932, -2.6225262 , -2.64832273, -2.19907796, -2.58734619,
                -2.3105317 , -2.54323491, -3.21585769, -2.30312854, -2.35617109,
                -2.50791723, -2.469056  , -2.56239095, -2.63982127, -2.63284791,
                -2.58846205, -2.41007734, -2.64763667, -2.59715948, -2.67384469,
                -2.86699985, -2.62522846, -2.67384469, -2.98184266, -2.59032303,
                -2.77013891, -2.85221108, -2.99829644, -2.4055141 , -2.20883295,
                -2.71566519, -2.53757337, -2.8403213 , -2.54268576, -2.70391231,
                 1.28479459,  0.93241075,  1.46406132,  0.18096721,  1.08713449,
                 0.64043675,  1.09522371, -0.75146714,  1.04329778, -0.01019007,
                -0.5110862 ,  0.51109806,  0.26233576,  0.98404455, -0.174864  ,
                 0.92757294,  0.65959279,  0.23454059,  0.94236171,  0.0432464 ,
                 1.11624072,  0.35678657,  1.29646885,  0.92050265,  0.71400821,
                 0.89964086,  1.33104142,  1.55739627,  0.81245555, -0.30733476,
                -0.07034289, -0.19188449,  0.13499495,  1.37873698,  0.58727485,
                 0.8072055 ,  1.22042897,  0.81286779,  0.24519516,  0.16451343,
                 0.46303099,  0.89016045,  0.22887905, -0.70708128,  0.35553304,
                 0.33112695,  0.37523823,  0.64169028, -0.90846333,  0.29780791,
                 2.53172698,  1.41407223,  2.61648461,  1.97081495,  2.34975798,
                 3.39687992,  0.51938325,  2.9320051 ,  2.31967279,  2.91813423,
                 1.66193495,  1.80234045,  2.16537886,  1.34459422,  1.5852673 ,
                 1.90474358,  1.94924878,  3.48876538,  3.79468686,  1.29832982,
                 2.42816726,  1.19809737,  3.49926548,  1.38766825,  2.27585365,
                 2.61419383,  1.25762518,  1.29066965,  2.12285398,  2.3875644 ,
                 2.84096093,  3.2323429 ,  2.15873837,  1.4431026 ,  1.77964011,
                 3.07652162,  2.14498686,  1.90486293,  1.16885347,  2.10765373,
                 2.31430339,  1.92245088,  1.41407223,  2.56332271,  2.41939122,
                 1.94401705,  1.52566363,  1.76404594,  1.90162908,  1.38966613])
\end{Verbatim}
            
    By plotting a pair plot of the first and second principal components
against each other, we can see that these principal components
adequately segregate the data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{c+c1}{\PYZsh{} Create a pairplot of each variable to identify if there is the class is easily distinguishable}
         \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{K}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}51}]:} <seaborn.axisgrid.PairGrid at 0x27e06c76828>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    By producing and pairs plot we can find the components showing the most
discrimination between the classes. Here we find that the first
principal component (principal component 0 in the plot below) produces
the most discrimination followed by the third principal component
(principal component 2 in the plot below).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{c+c1}{\PYZsh{} Create a pairplot of each principal component to identify if the class is easily distinguishable}
         \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{K}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{n}{plot\PYZus{}kws}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{color}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{n}{color}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}52}]:} <seaborn.axisgrid.PairGrid at 0x27e06a1c4a8>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_43_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{V}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{n}{iris\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}53}]:}                         PC1       PC2       PC3       PC4
         sepal length (cm)  0.361590 -0.656540 -0.580997  0.317255
         sepal width (cm)  -0.082269 -0.729712  0.596418 -0.324094
         petal length (cm)  0.856572  0.175767  0.072524 -0.479719
         petal width (cm)   0.358844  0.074706  0.549061  0.751121
\end{Verbatim}
            
    The principal components derived using the covariance matrix may give
rise to domination of a feature over others. An approach to negate the
effects of differing magnitudes amongst features is to use the
correlation matrix instead. The relationship between the covariance
matrix and the correlation matrix of \(\boldsymbol{X}\) is

\[cor(X_i,X_j) = \frac{cov(X_i,X_j)}{\sqrt{cov(X_i,X_i)cov(X_j,X_j)}}\]

where \(cov(X_i,X_i) = var(X_i)\)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{n}{C} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{matrix}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
         \PY{n}{C} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{corrcoef}\PY{p}{(}\PY{n}{C}\PY{p}{)}\PY{p}{)}
         \PY{n}{C} 
         
         \PY{n}{L}\PY{p}{,}\PY{n}{E} \PY{o}{=} \PY{n}{LA}\PY{o}{.}\PY{n}{eig}\PY{p}{(}\PY{n}{C}\PY{p}{)}
         \PY{n}{E} \PY{o}{=} \PY{n}{E}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Eigenvectors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{E}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Eigenvalues}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{L}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The diagonal matrix}
         \PY{n}{LI} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{identity}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{:}
             \PY{n}{LI}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{LI}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{n}{L}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{LI}\PY{p}{)}
         
         \PY{n}{V} \PY{o}{=} \PY{n}{E}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
         \PY{n}{K} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{V}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Create a pairplot of each principal component to identify if the class is easily distinguishable}
         \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{K}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{n}{plot\PYZus{}kws}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{color}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{n}{color}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{V}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{n}{iris\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Eigenvectors
[[ 0.52237162 -0.26335492  0.58125401  0.56561105]
 [-0.37231836 -0.92555649 -0.02109478 -0.06541577]
 [-0.72101681  0.24203288  0.14089226  0.6338014 ]
 [ 0.26199559 -0.12413481 -0.80115427  0.52354627]]
Eigenvalues
[2.91081808 0.92122093 0.14735328 0.02060771]
[[2.91081808 0.         0.         0.        ]
 [0.         0.92122093 0.         0.        ]
 [0.         0.         0.14735328 0.        ]
 [0.         0.         0.         0.02060771]]

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}82}]:}                         PC1       PC2       PC3       PC4
         sepal length (cm)  0.522372 -0.372318 -0.721017  0.261996
         sepal width (cm)  -0.263355 -0.925556  0.242033 -0.124135
         petal length (cm)  0.581254 -0.021095  0.140892 -0.801154
         petal width (cm)   0.565611 -0.065416  0.633801  0.523546
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_46_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    From the pair plot above, we see that principal components 1 and 4,
discriminate between the classes most. Looking at the principal
component dataframe above, shows that these two components have a
significant contribution from petal length and petal width. This is not
surprising since in our initial pair plot, these two features showed the
most discrimination.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}71}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{PCA}
         \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
         \PY{n}{pca}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}71}]:} PCA(copy=True, iterated\_power='auto', n\_components=4, random\_state=None,
           svd\_solver='auto', tol=0.0, whiten=False)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}72}]:} \PY{n}{pca}\PY{o}{.}\PY{n}{components\PYZus{}}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}72}]:} array([[ 0.36158968,  0.65653988, -0.58099728,  0.31725455],
                [-0.08226889,  0.72971237,  0.59641809, -0.32409435],
                [ 0.85657211, -0.1757674 ,  0.07252408, -0.47971899],
                [ 0.35884393, -0.07470647,  0.54906091,  0.75112056]])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}73}]:} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{pca}\PY{o}{.}\PY{n}{components\PYZus{}}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{n}{iris\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}73}]:}                         PC1       PC2       PC3       PC4
         sepal length (cm)  0.361590  0.656540 -0.580997  0.317255
         sepal width (cm)  -0.082269  0.729712  0.596418 -0.324094
         petal length (cm)  0.856572 -0.175767  0.072524 -0.479719
         petal width (cm)   0.358844 -0.074706  0.549061  0.751121
\end{Verbatim}
            
    The principal components obtained using sklearn.decomposition.PCA
matches our results. Note that if \(\boldsymbol{\alpha}\) is a vector
satisfying
\(\boldsymbol{X} \boldsymbol{\alpha} = \lambda \boldsymbol{\alpha}\) for
some \(\lambda\), then the following is also true:

\[\boldsymbol{X} (-\boldsymbol{\alpha}) = \lambda (-\boldsymbol{\alpha})\]

Hence, if \(\boldsymbol{\alpha}\) is an eigenvector, so is
\(-\boldsymbol{\alpha}\). Additionally, if \(\boldsymbol{\alpha}\) and
\(\boldsymbol{\beta}\) are two orthonormal vectors, then so are
\(-\boldsymbol{\alpha}\) and \(\boldsymbol{\beta}\). This explains why
some of the principal vectors obtained via sklearn.decomposition.PCA
differs from ours only by sign.

Below, we see that after fitting the PCA method, it is possible to
retrieve the covariance matrix which matches ours.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{pca}\PY{o}{.}\PY{n}{get\PYZus{}covariance}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{Z}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[[ 0.68569351 -0.03926846  1.27368233  0.5169038 ]
 [-0.03926846  0.18800403 -0.32171275 -0.11798121]
 [ 1.27368233 -0.32171275  3.11317942  1.29638747]
 [ 0.5169038  -0.11798121  1.29638747  0.58241432]]

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}81}]:}           0         1         2         3
         0  0.685694 -0.039268  1.273682  0.516904
         1 -0.039268  0.188004 -0.321713 -0.117981
         2  1.273682 -0.321713  3.113179  1.296387
         3  0.516904 -0.117981  1.296387  0.582414
\end{Verbatim}
            
    \subsubsection{A. Appendix}\label{a.-appendix}

    \paragraph{A1. The Variance of a Linear Combination of Random
Variables}\label{a1.-the-variance-of-a-linear-combination-of-random-variables}

    Let
\(\boldsymbol{X} = (\boldsymbol{X}_1,\boldsymbol{X}_2,...,\boldsymbol{X}_p)^T\)
be a column vector of random variables,
\(\boldsymbol{\alpha} = (\alpha_1, \alpha_2,..., \alpha_p)T\) be a
column vector of constants (i.e. \(\in R^p\)) and
\(P = \alpha_1 \boldsymbol{X}_1 + \alpha_2 \boldsymbol{X}_2 +...+\alpha_p \boldsymbol{X}_p = \sum_{i=1}^p \alpha_i \boldsymbol{X}_i\).
Then

\[\text{E}[P] = \text{E}[\sum_{i=1}^p \alpha_i \boldsymbol{X}_i] = \sum_{i=1}^p \alpha_i \text{E}[\boldsymbol{X}_i]\]
\[(\text{E}[P])^2 = \sum_{i=1}^p \alpha_i \text{E}[\boldsymbol{X}_i] \sum_{i=1}^p \alpha_i \text{E}[\boldsymbol{X}_i] = \sum_{i=1}^p \alpha_i^2 \text{E}[\boldsymbol{X}_i]^2 + \sum_{i \neq j, i,j \in \{1,2,..,p\}}^p \alpha_i \alpha_j \text{E}[\boldsymbol{X}_i] \text{E}[\boldsymbol{X}_j]\]
\[\text{E}[P^2] = \text{E}[(\sum_{i=1}^p \alpha_i \boldsymbol{X}_i)^2] = \text{E}[\sum_{i=1}^p \alpha_i^2 \boldsymbol{X}_i^2 + \sum_{i \neq j}^p \alpha_i \alpha_j \boldsymbol{X}_i \boldsymbol{X}_j] = \sum_{i=1}^p \alpha_i^2 \text{E}[\boldsymbol{X}_i^2] + \sum_{i \neq j}^p \alpha_i \alpha_j \text{E}[\boldsymbol{X}_i \boldsymbol{X}_j]\]

Putting these together

\[Var[P] = \text{E}[P^2] - \text{E}[P]^2 = \sum_{i=1}^p \alpha_i^2 (\text{E}[\boldsymbol{X}_i^2] - \text{E}[\boldsymbol{X}_i]^2) + \sum_{i \neq j}^p \alpha_i \alpha_j (\text{E}[\boldsymbol{X}_i \boldsymbol{X}_j] - \text{E}[\boldsymbol{X}_i] \text{E}[\boldsymbol{X}_j]) = \sum_{i=1}^p \alpha_i^2 Var[\boldsymbol{X}_i] + \sum_{i \neq j}^p \alpha_i \alpha_j Cov[\boldsymbol{X}_i,\boldsymbol{X}_j] = \sum_{i,j \in \{1,2,...,p\}} \alpha_i \alpha_j Cov[\boldsymbol{X}_i,\boldsymbol{X}_j] = \alpha^T \sum \alpha\]

where \(\sum_{ij} = Cov(\boldsymbol{X}_i,\boldsymbol{X}_j)\) is the
\(p \times p\) covariance matrix of \(\boldsymbol{X}\).


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
